{
    "batch_size" : 256,
    "gradient_accumulation_steps": 4,
    "mode":"wei",
    "dim" : 288,
    "n_layers" : 4,
    "max_seq_len" : 128,
    "n_heads" : 6,
    "vocab_size": 32000,
    "multiple_of" : 32, 
    "dropout": 0.0,
    "fsdp": true,
    "lr" : 5e-4,
    "iter_nums" : 100
}
