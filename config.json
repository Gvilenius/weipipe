{
    "batch_size": 64,
    "gradient_accumulation_steps": 2,
    "mode": "wei",
    "dim": 288,
    "n_layers": 4,
    "max_seq_len": 128,
    "n_heads": 6,
    "vocab_size": 32000,
    "multiple_of": 32,
    "dropout": 0.0,
    "fsdp": true,
    "lr": 0.0005,
    "iter_nums": 30,
    "output" : false
}
